{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hKNu7d9VemUw",
        "outputId": "c8cc2144-7111-41ae-e944-e8375ceee6ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scipy==1.13.1 in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Collecting statsmodels==0.14.2\n",
            "  Downloading statsmodels-0.14.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting matplotlib==3.7.1\n",
            "  Downloading matplotlib-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Collecting scikit-learn==1.5.1\n",
            "  Downloading scikit_learn-1.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting qiskit\n",
            "  Downloading qiskit-1.4.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting qiskit-aer\n",
            "  Downloading qiskit_aer-0.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels==0.14.2) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels==0.14.2) (24.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (1.4.8)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (3.2.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.1) (3.5.0)\n",
            "Collecting rustworkx>=0.15.0 (from qiskit)\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.11/dist-packages (from qiskit) (1.13.1)\n",
            "Collecting dill>=0.3 (from qiskit)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting stevedore>=3.0.0 (from qiskit)\n",
            "  Downloading stevedore-5.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from qiskit) (4.12.2)\n",
            "Collecting symengine<0.14,>=0.11 (from qiskit)\n",
            "  Downloading symengine-0.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.11/dist-packages (from qiskit-aer) (5.9.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
            "Collecting pbr>=2.0.0 (from stevedore>=3.0.0->qiskit)\n",
            "  Downloading pbr-6.1.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.3->qiskit) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pbr>=2.0.0->stevedore>=3.0.0->qiskit) (75.1.0)\n",
            "Downloading statsmodels-0.14.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit-1.4.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit_aer-0.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stevedore-5.4.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading symengine-0.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pbr-6.1.1-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: symengine, rustworkx, pbr, dill, stevedore, scikit-learn, matplotlib, statsmodels, qiskit, qiskit-aer\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.14.4\n",
            "    Uninstalling statsmodels-0.14.4:\n",
            "      Successfully uninstalled statsmodels-0.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dill-0.3.9 matplotlib-3.7.1 pbr-6.1.1 qiskit-1.4.0 qiskit-aer-0.16.1 rustworkx-0.16.0 scikit-learn-1.5.1 statsmodels-0.14.2 stevedore-5.4.1 symengine-0.13.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              },
              "id": "67ec9643ed93402cbca8cd372c615b78"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install required packages with specific compatible versions\n",
        "!pip install numpy==1.26.4 pandas==2.2.2 scipy==1.13.1 statsmodels==0.14.2 matplotlib==3.7.1 seaborn==0.13.2 scikit-learn==1.5.1 qiskit qiskit-aer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from statsmodels.tsa.stattools import acf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import os\n",
        "import time\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from qiskit import QuantumCircuit\n",
        "from qiskit_aer import AerSimulator\n",
        "import warnings\n",
        "\n",
        "# Suppress RuntimeWarnings\n",
        "warnings.filterwarnings('ignore', message='invalid value encountered in multiply')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prng_data(num_samples=10000, seed=42):\n",
        "    \"\"\"Generate PRNG data with 32-bit integers and doubles [0, 1].\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    integers = np.random.randint(0, 2**32, num_samples, dtype=np.uint32)\n",
        "    doubles = integers / (2**32 - 1)\n",
        "\n",
        "    start_time = time.time()\n",
        "    data = []\n",
        "    for i in range(num_samples):\n",
        "        timestamp = datetime.now().timestamp()\n",
        "        data.append({\n",
        "            'index': i,\n",
        "            'integer': int(integers[i]),\n",
        "            'double': float(doubles[i]),\n",
        "            'source': 'PRNG',\n",
        "            'timestamp': timestamp\n",
        "        })\n",
        "\n",
        "    output_file = 'prng_data.csv'\n",
        "    try:\n",
        "        with open(output_file, 'w', newline='') as csvfile:\n",
        "            fieldnames = ['index', 'integer', 'double', 'source', 'timestamp']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(data)\n",
        "        print(f\"PRNG data saved to {output_file} with {num_samples} samples\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error writing PRNG data: {e}\")\n",
        "        return\n",
        "\n",
        "    exec_time = time.time() - start_time\n",
        "    time_file = 'prng_time.txt'\n",
        "    try:\n",
        "        with open(time_file, 'w') as f:\n",
        "            f.write(str(exec_time))\n",
        "        print(f\"PRNG execution time saved to {time_file}: {exec_time:.2f} seconds\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error writing PRNG time: {e}\")\n",
        "\n",
        "    return exec_time\n",
        "\n",
        "# Generate PRNG data\n",
        "print(\"Generating PRNG data (10,000 samples)...\")\n",
        "prng_time = generate_prng_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqppycOAep9i",
        "outputId": "467218a9-d682-4dd0-9b2c-aa1131a0c0bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating PRNG data (10,000 samples)...\n",
            "PRNG data saved to prng_data.csv with 10000 samples\n",
            "PRNG execution time saved to prng_time.txt: 0.12 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_quantum_circuit(num_qubits):\n",
        "    \"\"\"Create a quantum circuit with Hadamard gates on all qubits.\"\"\"\n",
        "    qc = QuantumCircuit(num_qubits, num_qubits)\n",
        "    for i in range(num_qubits):\n",
        "        qc.h(i)  # Apply Hadamard gate\n",
        "    qc.measure_all()  # Measure all qubits\n",
        "    return qc\n",
        "\n",
        "def bits_to_integers(bitstring, bits_per_int=32):\n",
        "    \"\"\"Convert a bitstring into 32-bit integers.\"\"\"\n",
        "    integers = []\n",
        "    for i in range(0, len(bitstring) - bits_per_int + 1, bits_per_int):\n",
        "        chunk = bitstring[i:i + bits_per_int]\n",
        "        if len(chunk) == bits_per_int:\n",
        "            integers.append(int(chunk, 2))\n",
        "    return integers\n",
        "\n",
        "def integers_to_doubles(integers):\n",
        "    \"\"\"Convert 32-bit integers to doubles in [0, 1].\"\"\"\n",
        "    max_int = 2**32 - 1\n",
        "    return [x / max_int for x in integers]\n",
        "\n",
        "def run_quantum_simulator(num_samples=10000, num_qubits=6, shots_per_run=1000):\n",
        "    \"\"\"Generate quantum random numbers using Qiskit Aer simulator with optimized parameters.\"\"\"\n",
        "    start_time = time.time()\n",
        "    max_execution_time = 3600  # 1-hour cap\n",
        "\n",
        "    total_bits_needed = num_samples * 32\n",
        "    bits_per_run = num_qubits * shots_per_run\n",
        "    runs = int(np.ceil(total_bits_needed / bits_per_run))\n",
        "    print(f\"Using {num_qubits} qubits, {runs} runs, {shots_per_run} shots per run\")\n",
        "\n",
        "    simulator = AerSimulator()\n",
        "    all_bits = \"\"\n",
        "\n",
        "    for _ in range(runs):\n",
        "        if time.time() - start_time > max_execution_time:\n",
        "            print(\"Execution time limit of 1 hour reached.\")\n",
        "            break\n",
        "\n",
        "        qc = create_quantum_circuit(num_qubits)\n",
        "        job = simulator.run(qc, shots=shots_per_run)\n",
        "        result = job.result()\n",
        "        counts = result.get_counts()\n",
        "\n",
        "        bitstring = max(counts, key=counts.get).replace(\" \", \"\")\n",
        "        all_bits += bitstring * shots_per_run\n",
        "\n",
        "    bitstring = all_bits[:total_bits_needed]\n",
        "    integers = bits_to_integers(bitstring)\n",
        "    doubles = integers_to_doubles(integers)\n",
        "\n",
        "    if len(integers) < num_samples:\n",
        "        print(f\"Warning: Only generated {len(integers)} numbers instead of {num_samples}\")\n",
        "        additional = num_samples - len(integers)\n",
        "        pad_ints = np.random.randint(0, 2**32, additional, dtype=np.uint32)\n",
        "        pad_doubles = pad_ints / (2**32 - 1)\n",
        "        integers = np.concatenate([integers, pad_ints])\n",
        "        doubles = np.concatenate([doubles, pad_doubles])\n",
        "    elif len(integers) > num_samples:\n",
        "        integers = integers[:num_samples]\n",
        "        doubles = doubles[:num_samples]\n",
        "\n",
        "    data = []\n",
        "    for i in range(num_samples):\n",
        "        timestamp = datetime.now().timestamp()\n",
        "        data.append({\n",
        "            'index': i,\n",
        "            'integer': int(integers[i]),\n",
        "            'double': float(doubles[i]),\n",
        "            'source': 'Quantum',\n",
        "            'timestamp': timestamp\n",
        "        })\n",
        "\n",
        "    output_file = 'quantum_data.csv'\n",
        "    try:\n",
        "        with open(output_file, 'w', newline='') as csvfile:\n",
        "            fieldnames = ['index', 'integer', 'double', 'source', 'timestamp']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(data)\n",
        "        print(f\"Quantum data saved to {output_file} with {num_samples} samples\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error writing Quantum data: {e}\")\n",
        "        return\n",
        "\n",
        "    exec_time = time.time() - start_time\n",
        "    time_file = 'quantum_time.txt'\n",
        "    try:\n",
        "        with open(time_file, 'w') as f:\n",
        "            f.write(str(exec_time))\n",
        "        print(f\"Quantum execution time saved to {time_file}: {exec_time:.2f} seconds\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error writing Quantum time: {e}\")\n",
        "\n",
        "    return exec_time\n",
        "\n",
        "# Generate Quantum data with optimized parameters\n",
        "print(\"\\nGenerating Quantum simulated data (10,000 samples)...\")\n",
        "quantum_time = run_quantum_simulator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc8SJ6Bmeur9",
        "outputId": "cf43c19e-7b5f-400e-c8f5-527801736e8d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating Quantum simulated data (10,000 samples)...\n",
            "Using 6 qubits, 54 runs, 1000 shots per run\n",
            "Quantum data saved to quantum_data.csv with 10000 samples\n",
            "Quantum execution time saved to quantum_time.txt: 1.07 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bootstrapping and analysis functions\n",
        "def bootstrap_metric(data, metric_func, n_bootstraps=1000):\n",
        "    \"\"\"Perform bootstrapping on data to compute mean and 95% confidence intervals.\"\"\"\n",
        "    boot_results = []\n",
        "    for i in range(n_bootstraps):\n",
        "        sample = np.random.choice(data, size=len(data), replace=True)\n",
        "        try:\n",
        "            result = metric_func(sample)\n",
        "            if not np.isnan(result):\n",
        "                boot_results.append(result)\n",
        "            else:\n",
        "                print(f\"Warning: NaN result in bootstrap iteration {i} for {metric_func.__name__}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Chi-square failure in bootstrap iteration {i}: {e}\")\n",
        "            continue\n",
        "    if not boot_results:\n",
        "        return np.nan, (np.nan, np.nan)\n",
        "    if len(np.unique(boot_results)) == 1:\n",
        "        print(f\"Warning: Zero variance in bootstrapped results for {metric_func.__name__}\")\n",
        "        return boot_results[0], (np.nan, np.nan)\n",
        "    mean = np.mean(boot_results)\n",
        "    ci = stats.t.interval(0.95, df=len(boot_results)-1, loc=mean, scale=stats.sem(boot_results)) if len(boot_results) > 1 else (np.nan, np.nan)\n",
        "    return mean, ci\n",
        "\n",
        "def quality_score(entropy, chi2_p, autocorr):\n",
        "    \"\"\"Compute a quality score based on raw randomness metrics.\"\"\"\n",
        "    return 0.4 * entropy + 0.4 * chi2_p - 0.2 * np.abs(autocorr)\n",
        "\n",
        "def calculate_throughput(ints, exec_time):\n",
        "    \"\"\"Calculate throughput in bits per second.\"\"\"\n",
        "    return len(ints) * 32 / exec_time if exec_time else None\n",
        "\n",
        "def bootstrap_metrics(ints, doubles, bin_edges, expected):\n",
        "    \"\"\"Compute bootstrapped metrics for entropy, chi-square p-value, and autocorrelation.\"\"\"\n",
        "    # Entropy for integers\n",
        "    entropy_int_boot, entropy_int_ci = bootstrap_metric(\n",
        "        ints,\n",
        "        lambda x: -np.sum(\n",
        "            (p := np.bincount(np.frombuffer(x.astype(np.uint32).tobytes(), dtype=np.uint8), minlength=256) / len(x.astype(np.uint32).tobytes())) *\n",
        "            np.log2(np.where(p > 0, p, 1))\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Chi-square p-value for integers with stability check\n",
        "    chi2_p_boot, chi2_p_ci = bootstrap_metric(\n",
        "        ints,\n",
        "        lambda x: stats.chisquare(\n",
        "            np.histogram(x, bins=bin_edges)[0],\n",
        "            expected,\n",
        "            ddof=0\n",
        "        )[1] if len(np.unique(np.histogram(x, bins=bin_edges)[0])) > 1 else np.nan\n",
        "    )\n",
        "\n",
        "    # Autocorrelation (lag-1) for doubles\n",
        "    autocorr_boot, autocorr_ci = bootstrap_metric(\n",
        "        doubles,\n",
        "        lambda x: acf(x, nlags=1, fft=True)[1]\n",
        "    )\n",
        "\n",
        "    return (entropy_int_boot, entropy_int_ci), (chi2_p_boot, chi2_p_ci), (autocorr_boot, autocorr_ci)\n",
        "\n",
        "def compute_quality_scores(entropy_int, p_value, autocorr, entropy_int_boot, chi2_p_boot, autocorr_boot):\n",
        "    \"\"\"Compute original and bootstrapped quality scores.\"\"\"\n",
        "    quality = quality_score(entropy_int, p_value, autocorr)\n",
        "    quality_boot = quality_score(entropy_int_boot, chi2_p_boot, autocorr_boot)\n",
        "    return quality, quality_boot\n",
        "\n",
        "def collect_results(source, entropy_int, entropy_double, p_value, autocorr, throughput, exec_time,\n",
        "                    entropy_int_boot, entropy_int_ci, chi2_p_boot, chi2_p_ci, autocorr_boot, autocorr_ci,\n",
        "                    quality, quality_boot, results_list):\n",
        "    \"\"\"Append analysis results to the results list.\"\"\"\n",
        "    results_list.append([source, entropy_int, entropy_double, p_value, autocorr, throughput, exec_time, quality,\n",
        "                         entropy_int_boot, entropy_int_ci, chi2_p_boot, chi2_p_ci, autocorr_boot, autocorr_ci, quality_boot])\n",
        "\n",
        "# Load and analyze data\n",
        "files = ['prng_data.csv', 'quantum_data.csv']\n",
        "dataframes = []\n",
        "for f in files:\n",
        "    if os.path.exists(f):\n",
        "        df = pd.read_csv(f)\n",
        "        dataframes.append(df)\n",
        "        print(f\"Loaded {f} with {len(df)} samples\")\n",
        "    else:\n",
        "        print(f\"Warning: {f} not found, skipping.\")\n",
        "if not dataframes:\n",
        "    print(\"Error: No data files found.\")\n",
        "    exit(1)\n",
        "data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Read execution times\n",
        "exec_times = {}\n",
        "for method, fname in [('PRNG', 'prng_time.txt'), ('Quantum', 'quantum_time.txt')]:\n",
        "    if os.path.exists(fname):\n",
        "        with open(fname, 'r') as f:\n",
        "            exec_times[method] = float(f.read().strip())\n",
        "    else:\n",
        "        exec_times[method] = {'PRNG': 0.27, 'Quantum': 1.94}.get(method, None)\n",
        "        print(f\"Warning: {fname} not found, using fallback execution time: {exec_times[method]} seconds\")\n",
        "\n",
        "# Perform analysis\n",
        "results = []\n",
        "for source in ['PRNG', 'Quantum']:\n",
        "    subset = data[data['source'] == source]\n",
        "    ints = subset['integer'].values\n",
        "    doubles = subset['double'].values\n",
        "\n",
        "    # Original metrics\n",
        "    int_bytes = ints.astype(np.uint32).tobytes()\n",
        "    byte_counts = np.bincount(np.frombuffer(int_bytes, dtype=np.uint8), minlength=256)\n",
        "    p = byte_counts / len(int_bytes)\n",
        "    entropy_int = -np.sum(p[p > 0] * np.log2(p[p > 0]))\n",
        "\n",
        "    double_bytes = doubles.tobytes()\n",
        "    byte_counts = np.bincount(np.frombuffer(double_bytes, dtype=np.uint8), minlength=256)\n",
        "    p = byte_counts / len(double_bytes)\n",
        "    entropy_double = -np.sum(p[p > 0] * np.log2(p[p > 0]))\n",
        "\n",
        "    bin_edges = np.linspace(0, 2**32, 21)  # Increased to 20 bins for better resolution\n",
        "    observed, _ = np.histogram(ints, bins=bin_edges)\n",
        "    expected = np.full(20, len(ints) / 20)\n",
        "    chi2, p_value = stats.chisquare(observed, expected)\n",
        "\n",
        "    autocorr = acf(doubles, nlags=1, fft=True)[1]\n",
        "\n",
        "    exec_time = exec_times.get(source, None)\n",
        "    throughput = calculate_throughput(ints, exec_time)\n",
        "\n",
        "    (entropy_int_boot, entropy_int_ci), (chi2_p_boot, chi2_p_ci), (autocorr_boot, autocorr_ci) = bootstrap_metrics(ints, doubles, bin_edges, expected)\n",
        "    quality, quality_boot = compute_quality_scores(entropy_int, p_value, autocorr, entropy_int_boot, chi2_p_boot, autocorr_boot)\n",
        "    collect_results(source, entropy_int, entropy_double, p_value, autocorr, throughput, exec_time,\n",
        "                    entropy_int_boot, entropy_int_ci, chi2_p_boot, chi2_p_ci, autocorr_boot, autocorr_ci,\n",
        "                    quality, quality_boot, results)\n",
        "\n",
        "# Normalize quality scores\n",
        "scaler = MinMaxScaler()\n",
        "quality_values = [r[7] for r in results] + [r[14] for r in results]\n",
        "normalized_qualities = scaler.fit_transform(np.array(quality_values).reshape(-1, 1)).flatten()\n",
        "for i, r in enumerate(results):\n",
        "    r[7] = normalized_qualities[i]\n",
        "    r[14] = normalized_qualities[i + len(results)]\n",
        "\n",
        "# Save and display results\n",
        "columns = ['method', 'entropy_int', 'entropy_double', 'chi_square_p', 'autocorr_lag1', 'throughput', 'exec_time', 'quality',\n",
        "           'entropy_int_boot', 'entropy_int_ci', 'chi_square_p_boot', 'chi_square_p_ci', 'autocorr_lag1_boot', 'autocorr_lag1_ci', 'quality_boot']\n",
        "pd.DataFrame(results, columns=columns).to_csv('prng_quantum_analysis_summary.csv', index=False)\n",
        "\n",
        "# Plots\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, source in enumerate(['PRNG', 'Quantum']):\n",
        "    plt.hist(data[data['source'] == source]['integer'], bins=100, alpha=0.5, density=False, label=source, color=plt.cm.Set1(i))\n",
        "plt.legend()\n",
        "plt.title('Histogram of Integers: PRNG vs. Quantum')\n",
        "plt.xlabel('Integer Value (0–4,294,967,295)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.yscale('log')\n",
        "plt.savefig('prng_quantum_integers_hist_enhanced.png')\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, source in enumerate(['PRNG', 'Quantum']):\n",
        "    doubles = data[data['source'] == source]['double'].values\n",
        "    autocorr = acf(doubles, nlags=20, fft=True)\n",
        "    n = len(doubles)\n",
        "    ci = 1.96 / np.sqrt(n)\n",
        "    plt.plot(range(21), autocorr, label=source, color=plt.cm.Set1(i))\n",
        "    plt.fill_between(range(21), autocorr - ci, autocorr + ci, alpha=0.2, color=plt.cm.Set1(i))\n",
        "plt.legend()\n",
        "plt.title('Autocorrelation of Doubles: PRNG vs. Quantum (Lags 0–20)')\n",
        "plt.xlabel('Lag')\n",
        "plt.ylabel('Autocorrelation')\n",
        "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.savefig('prng_quantum_autocorr_plot_enhanced.png')\n",
        "plt.close()\n",
        "\n",
        "# Statistical tests\n",
        "from scipy.stats import kstest, uniform\n",
        "print(\"\\nKolmogorov-Smirnov Test for Uniformity (PRNG vs. Quantum):\")\n",
        "for source in ['PRNG', 'Quantum']:\n",
        "    subset = data[data['source'] == source]['integer'].values / 2**32\n",
        "    stat, p = kstest(subset, 'uniform')\n",
        "    print(f\"{source}: Statistic = {stat:.4f}, p-value = {p:.4f}\")\n",
        "\n",
        "print(\"\\nAnalysis Summary (PRNG vs. Quantum):\")\n",
        "for r in results:\n",
        "    print(f\"Method: {r[0]}\")\n",
        "    print(f\"  Entropy (int): {r[1]:.4f} bits/byte\")\n",
        "    print(f\"  Entropy (double): {r[2]:.4f} bits/byte\")\n",
        "    print(f\"  Chi-square p-value: {r[3]:.4f}\")\n",
        "    print(f\"  Autocorr lag-1: {r[4]:.4f}\")\n",
        "    print(f\"  Throughput: {r[5]:.2f} bits/second\")\n",
        "    print(f\"  Exec time: {r[6]:.2f} seconds\")\n",
        "    print(f\"  Quality score: {r[7]:.4f}\")\n",
        "    print(f\"  Bootstrapped Entropy (int): {r[8]:.4f} ({r[9][0]:.4f} - {r[9][1]:.4f})\")\n",
        "    print(f\"  Bootstrapped Chi-square p-value: {r[10]:.4f} ({r[11][0]:.4f} - {r[11][1]:.4f})\")\n",
        "    print(f\"  Bootstrapped Autocorr lag-1: {r[12]:.4f} ({r[13][0]:.4f} - {r[13][1]:.4f})\")\n",
        "    print(f\"  Bootstrapped Quality score: {r[14]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdo6RZ3aexmh",
        "outputId": "05084ba4-4986-4ea7-8c99-96b5b16f10bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded prng_data.csv with 10000 samples\n",
            "Loaded quantum_data.csv with 10000 samples\n",
            "Warning: Zero variance in bootstrapped results for <lambda>\n",
            "\n",
            "Kolmogorov-Smirnov Test for Uniformity (PRNG vs. Quantum):\n",
            "PRNG: Statistic = 0.0064, p-value = 0.8080\n",
            "Quantum: Statistic = 0.3551, p-value = 0.0000\n",
            "\n",
            "Analysis Summary (PRNG vs. Quantum):\n",
            "Method: PRNG\n",
            "  Entropy (int): 7.9956 bits/byte\n",
            "  Entropy (double): 7.4575 bits/byte\n",
            "  Chi-square p-value: 0.8072\n",
            "  Autocorr lag-1: -0.0009\n",
            "  Throughput: 2582197.66 bits/second\n",
            "  Exec time: 0.12 seconds\n",
            "  Quality score: 1.0000\n",
            "  Bootstrapped Entropy (int): 7.9910 (7.9910 - 7.9911)\n",
            "  Bootstrapped Chi-square p-value: 0.1058 (0.0954 - 0.1162)\n",
            "  Bootstrapped Autocorr lag-1: -0.0003 (-0.0009 - 0.0003)\n",
            "  Bootstrapped Quality score: 0.8360\n",
            "Method: Quantum\n",
            "  Entropy (int): 4.6380 bits/byte\n",
            "  Entropy (double): 5.3063 bits/byte\n",
            "  Chi-square p-value: 0.0000\n",
            "  Autocorr lag-1: -0.2784\n",
            "  Throughput: 300155.53 bits/second\n",
            "  Exec time: 1.07 seconds\n",
            "  Quality score: 0.0000\n",
            "  Bootstrapped Entropy (int): 4.6372 (4.6369 - 4.6376)\n",
            "  Bootstrapped Chi-square p-value: 0.0000 (nan - nan)\n",
            "  Bootstrapped Autocorr lag-1: -0.0008 (-0.0015 - -0.0002)\n",
            "  Bootstrapped Quality score: 0.0321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NcE1p2N6fgRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages in Colab with compatible versions\n",
        "!pip install qiskit==1.4.0 qiskit-ibm-runtime==0.36.1 numpy==1.26.4\n",
        "\n",
        "import numpy as np\n",
        "from qiskit import QuantumCircuit, transpile\n",
        "from qiskit_ibm_runtime import QiskitRuntimeService, Batch, SamplerV2 as Sampler\n",
        "import time\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Authenticate with IBM Quantum API token\n",
        "api_token = 'YOUR_API_TOKEN'  # Replace with your actual IBM Quantum API token from quantum-computing.ibm.com\n",
        "try:\n",
        "    service = QiskitRuntimeService(channel=\"ibm_quantum\", token=api_token)\n",
        "    print(\"IBM Runtime Service loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Authentication failed: {e}\")\n",
        "    print(\"Ensure your API token is correct and pasted above.\")\n",
        "    raise\n",
        "\n",
        "def create_quantum_circuit(num_qubits):\n",
        "    \"\"\"Create a quantum circuit with Hadamard gates on all qubits.\"\"\"\n",
        "    qc = QuantumCircuit(num_qubits)\n",
        "    for i in range(num_qubits):\n",
        "        qc.h(i)  # Apply Hadamard gate\n",
        "    qc.measure_all()  # Measure all qubits\n",
        "    return qc\n",
        "\n",
        "def bits_to_integers(bitstring, bits_per_int=32):\n",
        "    \"\"\"Convert a bitstring into 32-bit integers.\"\"\"\n",
        "    integers = []\n",
        "    for i in range(0, len(bitstring) - bits_per_int + 1, bits_per_int):\n",
        "        chunk = bitstring[i:i + bits_per_int]\n",
        "        if len(chunk) == bits_per_int:\n",
        "            integers.append(int(chunk, 2))\n",
        "    return integers\n",
        "\n",
        "def integers_to_doubles(integers):\n",
        "    \"\"\"Convert 32-bit integers to doubles in [0, 1].\"\"\"\n",
        "    max_int = 2**32 - 1\n",
        "    return [x / max_int for x in integers]\n",
        "\n",
        "def run_quantum_hardware_qrng(num_samples=500, num_qubits=1, shots_per_job=250, num_circuits=64):\n",
        "    \"\"\"Generate 500 32-bit quantum random numbers using IBM Quantum hardware in batch mode.\"\"\"\n",
        "    start_time = time.time()\n",
        "    max_execution_time = 600  # 600 seconds cap per job\n",
        "    total_bits_needed = num_samples * 32  # 16,000 bits for 500 samples\n",
        "\n",
        "    print(f\"Batch run: {num_circuits} circuits, {num_qubits} qubit each, {shots_per_job} shots per circuit\")\n",
        "    print(f\"Generating {total_bits_needed} bits for {num_samples} 32-bit integers.\")\n",
        "    print(\"Queue times may extend beyond 600 seconds.\")\n",
        "\n",
        "    # Select a backend\n",
        "    try:\n",
        "        backend = service.least_busy(operational=True, simulator=False)\n",
        "        print(f\"Selected backend: {backend.name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error selecting backend: {e}\")\n",
        "        print(\"List available backends with: [b.name for b in service.backends()]\")\n",
        "        return\n",
        "\n",
        "    # Generate and transpile circuits\n",
        "    circuits = [create_quantum_circuit(num_qubits) for _ in range(num_circuits)]\n",
        "    transpiled_circuits = [transpile(circ, backend=backend, optimization_level=3) for circ in circuits]\n",
        "    print(\"Circuits transpiled successfully.\")\n",
        "\n",
        "    all_bits = \"\"\n",
        "    jobs = []\n",
        "\n",
        "    # Run jobs in batch mode\n",
        "    try:\n",
        "        with Batch(backend=backend, max_time=\"10m\"):  # 10-minute max TTL for the batch\n",
        "            sampler = Sampler()\n",
        "            for i, transpiled_qc in enumerate(transpiled_circuits):\n",
        "                job = sampler.run([transpiled_qc], shots=shots_per_job)\n",
        "                jobs.append(job)\n",
        "                print(f\"Job {i+1} submitted: {job.job_id()}\")\n",
        "\n",
        "        # Wait for all jobs to complete and collect results\n",
        "        for i, job in enumerate(jobs):\n",
        "            result = job.result(timeout=max_execution_time)\n",
        "            counts = result[0].data.meas.get_counts()\n",
        "            print(f\"Job {i+1} completed with counts: {counts}\")\n",
        "            for bitstring in counts:\n",
        "                all_bits += bitstring * counts[bitstring]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Batch execution failed: {e}\")\n",
        "        if \"timeout\" in str(e).lower():\n",
        "            print(\"Execution exceeded 600 seconds; check job status on IBM Quantum dashboard.\")\n",
        "        elif \"quota\" in str(e).lower():\n",
        "            print(\"Quota exceeded; wait for daily refresh or check IBM Quantum dashboard.\")\n",
        "        return\n",
        "\n",
        "    # Verify bitstring length\n",
        "    print(f\"Raw bitstring length: {len(all_bits)} bits\")\n",
        "    if len(all_bits) < total_bits_needed:\n",
        "        print(f\"Warning: Only got {len(all_bits)} bits, needed {total_bits_needed}. Padding with zeros.\")\n",
        "\n",
        "    # Process bits into integers and doubles\n",
        "    bitstring = all_bits + \"0\" * (total_bits_needed - len(all_bits))  # Pad if needed\n",
        "    integers = bits_to_integers(bitstring)\n",
        "    doubles = integers_to_doubles(integers)\n",
        "\n",
        "    # Ensure exactly 500 numbers\n",
        "    print(f\"Generated {len(integers)} integers\")\n",
        "    if len(integers) != num_samples:\n",
        "        print(f\"Error: Expected {num_samples} integers, got {len(integers)}. Results may be incomplete.\")\n",
        "\n",
        "    # Prepare data for CSV\n",
        "    data = []\n",
        "    for i in range(num_samples):\n",
        "        data.append({\n",
        "            'index': i,\n",
        "            'integer': integers[i] if i < len(integers) else 0,\n",
        "            'double': doubles[i] if i < len(doubles) else 0.0,\n",
        "            'source': 'Quantum_Hardware',\n",
        "            'timestamp': datetime.now().timestamp() + (i * 1e-6)  # Increment timestamp slightly\n",
        "        })\n",
        "\n",
        "    # Save to CSV\n",
        "    output_file = 'quantum_hardware_data.csv'\n",
        "    try:\n",
        "        with open(output_file, 'w', newline='') as csvfile:\n",
        "            fieldnames = ['index', 'integer', 'double', 'source', 'timestamp']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(data)\n",
        "        print(f\"Hardware Quantum data saved to {output_file}\")\n",
        "        print(\"To download, check the Files tab on the left in Colab.\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error writing to file: {e}\")\n",
        "        return\n",
        "\n",
        "    # Calculate and display metrics\n",
        "    execution_time = time.time() - start_time\n",
        "    throughput = total_bits_needed / execution_time if execution_time > 0 else 0\n",
        "    print(f\"Execution time (submission to result): {execution_time:.2f} seconds\")\n",
        "    print(f\"Throughput: {throughput:.2f} bits/second\")\n",
        "    print(\"Run complete! Compare with simulator and PRNG data for analysis.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        run_quantum_hardware_qrng()\n",
        "    except Exception as e:\n",
        "        print(f\"An unanticipated error occurred: {e}\")"
      ],
      "metadata": {
        "id": "AX2t-pdsfzYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages for analysis\n",
        "!pip install numpy==1.26.4 pandas==2.2.2 scipy==1.13.1 statsmodels==0.14.2 matplotlib==3.7.1 seaborn==0.13.2 scikit-learn==1.5.1\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from statsmodels.tsa.stattools import acf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress RuntimeWarnings\n",
        "warnings.filterwarnings('ignore', message='invalid value encountered in multiply')\n",
        "\n",
        "# Load CSV files for PRNG, Simulated Quantum, and Hardware Quantum\n",
        "files = ['prng_data.csv', 'quantum_data.csv', 'quantum_hardware_data.csv']\n",
        "dataframes = []\n",
        "for f in files:\n",
        "    if os.path.exists(f):\n",
        "        df = pd.read_csv(f)\n",
        "        # Truncate or pad to 500 samples for consistency (matching hardware output)\n",
        "        if len(df) > 500:\n",
        "            df = df.iloc[:500]\n",
        "        elif len(df) < 500:\n",
        "            additional = 500 - len(df)\n",
        "            df = pd.concat([df, df.sample(n=additional, replace=True, random_state=42)], ignore_index=True)\n",
        "        dataframes.append(df)\n",
        "        print(f\"Loaded {f} with {len(df)} samples (adjusted to 500)\")\n",
        "    else:\n",
        "        print(f\"Warning: {f} not found, skipping.\")\n",
        "if not dataframes:\n",
        "    print(\"Error: No data files found.\")\n",
        "    exit(1)\n",
        "data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Read execution times with fallbacks\n",
        "exec_times = {\n",
        "    'PRNG': None,\n",
        "    'Quantum': None,\n",
        "    'Quantum_Hardware': None\n",
        "}\n",
        "for method, fname in [('PRNG', 'prng_time.txt'), ('Quantum', 'quantum_time.txt'), ('Quantum_Hardware', 'quantum_hardware_time.txt')]:\n",
        "    if os.path.exists(fname):\n",
        "        with open(fname, 'r') as f:\n",
        "            exec_times[method] = float(f.read().strip())\n",
        "    else:\n",
        "        exec_times[method] = {'PRNG': 0.27, 'Quantum': 7.8, 'Quantum_Hardware': 280.34}.get(method, None)\n",
        "        print(f\"Warning: {fname} not found, using fallback: {exec_times[method]} seconds\")\n",
        "\n",
        "# Bootstrapping function\n",
        "def bootstrap_metric(data, metric_func, n_bootstraps=1000):\n",
        "    \"\"\"Perform bootstrapping on data to compute mean and 95% confidence intervals.\"\"\"\n",
        "    boot_results = []\n",
        "    for _ in range(n_bootstraps):\n",
        "        sample = np.random.choice(data, size=len(data), replace=True)\n",
        "        try:\n",
        "            result = metric_func(sample)\n",
        "            if not np.isnan(result):\n",
        "                boot_results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Bootstrap failure for {metric_func.__name__}: {e}\")\n",
        "            continue\n",
        "    if not boot_results:\n",
        "        return np.nan, (np.nan, np.nan)\n",
        "    mean = np.mean(boot_results)\n",
        "    ci = stats.t.interval(0.95, df=len(boot_results)-1, loc=mean, scale=stats.sem(boot_results)) if len(boot_results) > 1 else (np.nan, np.nan)\n",
        "    return mean, ci\n",
        "\n",
        "# Quality score function\n",
        "def quality_score(entropy, chi2_p, autocorr):\n",
        "    \"\"\"Compute a quality score based on raw randomness metrics.\"\"\"\n",
        "    return 0.4 * entropy + 0.4 * chi2_p - 0.2 * np.abs(autocorr)\n",
        "\n",
        "# Analysis functions\n",
        "def calculate_throughput(ints, exec_time):\n",
        "    \"\"\"Calculate throughput in bits per second.\"\"\"\n",
        "    return len(ints) * 32 / exec_time if exec_time else None\n",
        "\n",
        "def analyze_method(source, ints, doubles, exec_time):\n",
        "    # Entropy for integers\n",
        "    int_bytes = ints.astype(np.uint32).tobytes()\n",
        "    p = np.bincount(np.frombuffer(int_bytes, dtype=np.uint8), minlength=256) / len(int_bytes)\n",
        "    entropy_int = -np.sum(p[p > 0] * np.log2(p[p > 0]))\n",
        "\n",
        "    # Chi-square test (20 bins for better resolution)\n",
        "    bin_edges = np.linspace(0, 2**32, 21)\n",
        "    observed, _ = np.histogram(ints, bins=bin_edges)\n",
        "    expected = np.full(20, len(ints) / 20)\n",
        "    chi2, p_value = stats.chisquare(observed, expected)\n",
        "\n",
        "    # Autocorrelation (lag-1)\n",
        "    autocorr = acf(doubles, nlags=1, fft=True)[1]\n",
        "\n",
        "    # Throughput\n",
        "    throughput = calculate_throughput(ints, exec_time)\n",
        "\n",
        "    # Bootstrapped metrics\n",
        "    entropy_boot, entropy_ci = bootstrap_metric(\n",
        "        ints,\n",
        "        lambda x: -np.sum(\n",
        "            (p := np.bincount(np.frombuffer(x.astype(np.uint32).tobytes(), dtype=np.uint8), minlength=256) / len(x.astype(np.uint32).tobytes())) *\n",
        "            np.log2(np.where(p > 0, p, 1))\n",
        "        )\n",
        "    )\n",
        "    chi2_p_boot, chi2_p_ci = bootstrap_metric(\n",
        "        ints,\n",
        "        lambda x: stats.chisquare(np.histogram(x, bins=bin_edges)[0], expected)[1]\n",
        "    )\n",
        "    autocorr_boot, autocorr_ci = bootstrap_metric(\n",
        "        doubles,\n",
        "        lambda x: acf(x, nlags=1, fft=True)[1]\n",
        "    )\n",
        "\n",
        "    # Quality scores\n",
        "    quality = quality_score(entropy_int, p_value, autocorr)\n",
        "    quality_boot = quality_score(entropy_boot, chi2_p_boot, autocorr_boot)\n",
        "\n",
        "    return [source, entropy_int, p_value, autocorr, throughput, exec_time, quality,\n",
        "            entropy_boot, entropy_ci, chi2_p_boot, chi2_p_ci, autocorr_boot, autocorr_ci, quality_boot]\n",
        "\n",
        "# Perform analysis\n",
        "results = []\n",
        "for source in ['PRNG', 'Quantum', 'Quantum_Hardware']:\n",
        "    subset = data[data['source'] == source]\n",
        "    ints = subset['integer'].values\n",
        "    doubles = subset['double'].values\n",
        "    exec_time = exec_times.get(source)\n",
        "    result = analyze_method(source, ints, doubles, exec_time)\n",
        "    results.append(result)\n",
        "\n",
        "# Normalize quality scores across all methods\n",
        "scaler = MinMaxScaler()\n",
        "quality_values = [r[6] for r in results] + [r[13] for r in results]\n",
        "normalized_qualities = scaler.fit_transform(np.array(quality_values).reshape(-1, 1)).flatten()\n",
        "for i, r in enumerate(results):\n",
        "    r[6] = normalized_qualities[i]\n",
        "    r[13] = normalized_qualities[i + len(results)]\n",
        "\n",
        "# Save results\n",
        "columns = ['method', 'entropy_int', 'chi_square_p', 'autocorr_lag1', 'throughput', 'exec_time', 'quality',\n",
        "           'entropy_boot', 'entropy_ci', 'chi2_p_boot', 'chi2_p_ci', 'autocorr_boot', 'autocorr_ci', 'quality_boot']\n",
        "pd.DataFrame(results, columns=columns).to_csv('prng_quantum_analysis_summary.csv', index=False)\n",
        "\n",
        "# Visualize results\n",
        "# Histogram of Integers\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, source in enumerate(['PRNG', 'Quantum', 'Quantum_Hardware']):\n",
        "    plt.hist(data[data['source'] == source]['integer'], bins=100, alpha=0.5, density=False, label=source, color=plt.cm.Set1(i))\n",
        "plt.legend()\n",
        "plt.title('Histogram of Integers: PRNG vs. Quantum (Simulated & Hardware)')\n",
        "plt.xlabel('Integer Value (0–4,294,967,295)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.yscale('log')\n",
        "plt.savefig('prng_quantum_integers_hist_enhanced.png')\n",
        "plt.close()\n",
        "\n",
        "# Autocorrelation Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, source in enumerate(['PRNG', 'Quantum', 'Quantum_Hardware']):\n",
        "    doubles = data[data['source'] == source]['double'].values\n",
        "    autocorr = acf(doubles, nlags=20, fft=True)\n",
        "    n = len(doubles)\n",
        "    ci = 1.96 / np.sqrt(n)\n",
        "    plt.plot(range(21), autocorr, label=source, color=plt.cm.Set1(i))\n",
        "    plt.fill_between(range(21), autocorr - ci, autocorr + ci, alpha=0.2, color=plt.cm.Set1(i))\n",
        "plt.legend()\n",
        "plt.title('Autocorrelation of Doubles: PRNG vs. Quantum (Lags 0–20)')\n",
        "plt.xlabel('Lag')\n",
        "plt.ylabel('Autocorrelation')\n",
        "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.savefig('prng_quantum_autocorr_plot_enhanced.png')\n",
        "plt.close()\n",
        "\n",
        "# Statistical tests\n",
        "print(\"\\nKolmogorov-Smirnov Test for Uniformity (PRNG vs. Quantum):\")\n",
        "for source in ['PRNG', 'Quantum', 'Quantum_Hardware']:\n",
        "    subset = data[data['source'] == source]['integer'].values / 2**32\n",
        "    stat, p = stats.kstest(subset, 'uniform')\n",
        "    print(f\"{source}: Statistic = {stat:.4f}, p-value = {p:.4f}\")\n",
        "\n",
        "print(\"\\nAnalysis Summary (PRNG vs. Quantum):\")\n",
        "for r in results:\n",
        "    print(f\"Method: {r[0]}\")\n",
        "    print(f\"  Entropy (int): {r[1]:.4f} bits/byte\")\n",
        "    print(f\"  Chi-square p-value: {r[2]:.4f}\")\n",
        "    print(f\"  Autocorr lag-1: {r[3]:.4f}\")\n",
        "    print(f\"  Throughput: {r[4]:.2f} bits/second\" if r[4] else \"  Throughput: N/A\")\n",
        "    print(f\"  Exec time: {r[5]:.2f} seconds\" if r[5] else \"  Exec time: N/A\")\n",
        "    print(f\"  Quality score: {r[6]:.4f}\")\n",
        "    print(f\"  Bootstrapped Entropy (int): {r[7]:.4f} ({r[8][0]:.4f} - {r[8][1]:.4f})\")\n",
        "    print(f\"  Bootstrapped Chi-square p-value: {r[9]:.4f} ({r[10][0]:.4f} - {r[10][1]:.4f})\")\n",
        "    print(f\"  Bootstrapped Autocorr lag-1: {r[11]:.4f} ({r[12][0]:.4f} - {r[12][1]:.4f})\")\n",
        "    print(f\"  Bootstrapped Quality score: {r[13]:.4f}\")"
      ],
      "metadata": {
        "id": "88ptpPEtf4A5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}